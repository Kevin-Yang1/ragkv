#!/bin/bash
set -euo pipefail
# =============================================================================
# LongBench 数据集评估脚本
# =============================================================================
# 功能：在 LongBench 数据集上评估不同模型的长上下文理解能力
# 支持：多种 token 选择策略（reuse）和 KV Cache 驱逐策略（drop）
# 输出：每个数据集的评估结果保存到 ./outputs/ 目录
# =============================================================================

# -----------------------------------------------------------------------------
# 环境配置
# -----------------------------------------------------------------------------
# 设置 HuggingFace 镜像源（加速国内下载）
export HF_ENDPOINT=https://hf-mirror.com

# 设置 CUDA 架构版本（根据 GPU 型号调整）
# 8.6 对应 RTX 3090/A6000，8.0 对应 A100，9.0 对应 H100
export TORCH_CUDA_ARCH_LIST="8.6"

# -----------------------------------------------------------------------------
# 模型路径配置
# -----------------------------------------------------------------------------
# 定义模型名称到实际路径的映射
# 格式：path_map['模型简称']='模型实际路径'
declare -A path_map
path_map['Mistral-7B-Instruct']='../Models/LLMs/Mistral-7B-Instruct-v0.2'
path_map['Llama-3-8B-Instruct']='/data/ykw/models/Meta-Llama-3-8B-Instruct'
path_map['Qwen2.5-7B-Instruct']='../Models/LLMs/Qwen2.5-7B-Instruct'

# 如果使用 Llama 3.1，可以添加：
# path_map['Llama-3.1-8B-Instruct']='../Models/LLMs/llama3.1/Meta-Llama-3.1-8B-Instruct'

# =============================================================================
# 主要配置区域 - 根据实验需求修改这里
# =============================================================================

# -----------------------------------------------------------------------------
# GPU 配置
# -----------------------------------------------------------------------------
# 指定使用的 GPU 编号（0,1,2... 或多卡如 0,1）
export CUDA_VISIBLE_DEVICES=1

# -----------------------------------------------------------------------------
# 模型选择
# -----------------------------------------------------------------------------
# 选择要评估的模型（取消注释想用的模型）
# model='Mistral-7B-Instruct'
model='Llama-3-8B-Instruct'
# model='Qwen2.5-7B-Instruct'

# -----------------------------------------------------------------------------
# Token 选择策略配置（Reuse Strategy）
# -----------------------------------------------------------------------------
# 定义要测试的 token 选择策略列表
# 可以添加多个策略进行对比实验
declare -a reuse_list=(
    'debug'      # A³ 核心算法（基于注意力分数选择）- 推荐
    # 'surprisal_chunk' # 基于离线 token 惊奇度（按 chunk 分配预算）
    # 'blend'    # 基于 Value 差异选择
    # 'full'     # 仅保留末尾 tokens（基线对照）
    # 'attnlink' # 使用 Sink tokens
    # 'cat'      # 极限压缩（仅保留最后 1 个 token）
)

# -----------------------------------------------------------------------------
# KV Cache 驱逐策略配置（Drop Strategy）
# -----------------------------------------------------------------------------
# 选择 KV Cache 驱逐策略（可选）
# 用于在生成阶段管理 KV Cache 内存

# drop='Streaming'  # Streaming LLM 驱逐策略
# drop='SnapKV'     # SnapKV 驱逐策略
# drop_config='./config/drop/snap1.json'  # 驱逐策略配置文件

drop='False'  # 不使用驱逐策略（默认）

# =============================================================================
# 以下为自动执行部分，一般无需修改
# =============================================================================

# -----------------------------------------------------------------------------
# 数据集配置
# -----------------------------------------------------------------------------
# LongBench 基准测试包含的数据集列表
# 涵盖多种长上下文任务：问答、摘要、分类等
declare -a dataset_list=(
    "qasper"          # 科学论文问答
    "multifieldqa_en" # 多领域问答（英文）
    "hotpotqa"        # 多跳推理问答
    "2wikimqa"        # Wikipedia 多跳问答
    "gov_report"      # 政府报告摘要
    "multi_news"      # 多文档新闻摘要
    "trec"            # TREC 问题分类
    "triviaqa"        # 琐事问答
    "samsum"          # 对话摘要
    "passage_count"   # 段落计数
    "lcc"             # 长上下文代码补全
)
dataset_list=("2wikimqa")

# 可选：添加更多数据集
# "narrativeqa"     # 叙事问答
# "qmsum"           # 会议摘要
# "musique"         # 多跳推理

# -----------------------------------------------------------------------------
# 主循环：遍历策略和数据集进行评估
# -----------------------------------------------------------------------------
# 外层循环：遍历所有 token 选择策略
for reuse in "${reuse_list[@]}"; do
    echo "========================================="
    echo "开始评估策略: ${reuse}"
    echo "========================================="
    
    # 内层循环：遍历所有数据集
    for dataset in "${dataset_list[@]}"; do
        echo ">>> 正在评估数据集: ${dataset}"
        
        # 根据是否使用驱逐策略设置输出目录
        if [ ${drop} != "False" ]; then
            # 使用驱逐策略：输出路径包含策略名称
            output_dir=./outputs/${model}/${reuse}-${drop}/${dataset}
        else
            # 不使用驱逐策略：仅包含 reuse 策略名称
            drop_config=None
            output_dir=./outputs/${model}/${reuse}/${dataset}
        fi
        
        # 创建输出目录（如果不存在）
        mkdir -p ${output_dir}
        
        # ---------------------------------------------------------------------
        # 执行评估脚本
        # ---------------------------------------------------------------------
        python ./eval_longbench.py \
            --model ${path_map[$model]} \
            --reuse ${reuse} \
            --output_path ${output_dir} \
            --dataset ${dataset} \
            --kv_path ./kvs/${model}/${dataset} \
            --drop ${drop} \
            --drop_config ${drop_config}

        # 等待当前评估完成再继续下一个
        wait
        
        echo ">>> ${dataset} 评估完成！"
    done
    
    echo "策略 ${reuse} 所有数据集评估完成！"
    echo ""
done

echo "========================================="
echo "所有评估任务完成！"
echo "结果保存在: ./outputs/${model}/"
echo "========================================="
